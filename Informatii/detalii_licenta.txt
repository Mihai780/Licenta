Visualization of Attention

Attention model:
At each decoding step, we start with the encoder outputs, a set of 196 region vectors (one for each 14×14 spatial location) and the decoder’s previous hidden state. We need to score each region i by measuring how well it “fits” with what the decoder already knows.

We first project both the 1792-dimensional region vectors and the 512-dimensional hidden state into a common “attention space.”
Each 1792-dim region vector ai​ comes from the CNN and encodes local visual patterns (textures, shapes, colors) at spatial location i.
The 512-dim hidden state ht−1 comes from the LSTM and encodes language context—what words have been generated so far, grammar, sentence structure.

In code, that’s
	att1 = encoder_att(encoder_out)        # (batch, 196, attention_dim)
	att2 = decoder_att(decoder_hidden)     # (batch, attention_dim)
By bringing them into the same space, we can directly compare a region ai​ with the decoder state ht−1.
The LSTM’s cell state ct​ is essentially the network’s “memory”—the part that can carry information forward over many time steps without it being immediately squashed by nonlinearities. It works together with the hidden state ht to control what information gets stored, forgotten, or exposed to the next layer.

For each region i, we first measure compatibility by adding its projected vector a~i to the projected hidden state h~ and applying a ReLU nonlinearity. Concretely, zi​=ReLU(a~i​+h~).
This addition tells us how well that patch fits what the decoder currently needs, and the ReLU “switch” lets the model learn nonlinear matching functions rather than being limited to a simple dot-product.
Next, we collapse each combined vector down to a single score eiei​ by applying a linear layer with weight vector vv. In code this is
	e = full_att(z)  # z has shape (batch, 196, attention_dim)
	e = e.squeeze(2) # now (batch, 196)
This is equivalent to computing ei=v⊤zi​+b for each region, producing one scalar “energy” per patch.
We then normalize these raw energies into a probability distribution {αi} via softmax:
  alfai=(exp(ei))/(suma de j=1 la 196 exp(ej))
The softmax ensures the αi​ sum to 1, so the highest-scoring patches get the largest attention weights, but every region still contributes something.
With those attention weights in hand, we build the context vector z^ as the weighted sum of the original encoder features:
  z^=sum de la i=1 la 196 alfai * ai
This single vector is a concise summary of the “most relevant” parts of the image for the current decoding step—it compresses complex visual information into one feature.
Finally, we use and visualize these results. The context vector z^ (often after passing through a small gate) is fed into the LSTM to produce the next hidden state and predict the next word. Meanwhile, we keep the αiαi​ values so we can overlay them as a heatmap on the image, clearly showing which regions the model “looked at” when generating each word.

Decoder:

Initial weights and bias:
    You initialize all LSTM weight matrices (the WWs and UUs) randomly in a small range (e.g.\ uniform [−0.1,0.1][−0.1,0.1]) so that each gate starts with unique parameters—otherwise they’d receive identical gradients and remain the same.
    You set all biases bf,bi,bc,bobf​,bi​,bc​,bo​ to zero so there’s no built-in preference for “always remember” or “always forget” at the start.
    Keeping these initial values small helps avoid vanishing or exploding gradients during backpropagation, letting the network learn balanced gate behaviors from the data
    
Avem nebunia aia de desen iar zt este context vector calculat de attention, E yt-1 este cuvantul ales ultimul care este luat din matricea E care e embedded, ct-1 este cell state-ul, adica memoria iar ht-1 este hidden state adica ce tine minte LSTM-ul ca a generat pana acuma). Avem forget gate care decide cat de mult din ct-1 sa pastram, input gate controleaza cata informatie noua sa fie scrisa in cell, input moderator propune un content nou de memoria bazandu-se pe aceleasi inputuri. La initial weights, pune bias la 0 sa nu aiba vreo preferinta pentru un cuvant,trb sa fie initializati random ca sa aiba identitate proprie ca sa invete featurile diferite altfel in timpul la backpropagation ar primii aceiasi gradients si ar sta identici pentru totdeauna si sunt valori mici ca sa evitam exploding/vanishing gradients

Antrenare:
Ce este o epoca?
An epoch is one complete pass through your entire training dataset. When you train in mini-batches, you break your data into smaller chunks (say, 32 or 64 images at a time). Processing one of those chunks and updating the model’s weights is called an iteration (or a step).
Once you’ve run through all of those mini-batches—i.e. seen every training example exactly once—that counts as one epoch.

Ce este un mini-batch?
When you train a neural network, you rarely feed it your entire dataset in one go—that would be slow and memory‐hungry. Instead, you split your training set into many small groups of examples called mini-batches (or just “batches”).

Averagemeter?
and internally it maintains
    .val (the most‐recent batch’s value),
    .sum (the cumulative total of val * n),
    .count(the cumulative n), and
    .avg (the running average = sum / count).
Why you use it: it gives you a stable, epoch‐to‐date view of your key metrics—batch time, data‐loading time, loss per token, and top-5 accuracy—so your training logs show smoothed values rather than wildly fluctuating single‐batch numbers.

Why you use it: decaying the learning rate when progress stalls helps the optimizer take smaller, more precise steps, often unlocking further gains—especially important late in training when you’re trying to squeeze out the last bits of BLEU.

Accuracy:
which finds the model’s top-5 predicted tokens for each true next-word, counts how often the ground truth is among those five, and reports that percentage.
Why you use it: BLEU is a sequence‐level metric that you only compute once per epoch (on the entire validation set). Meanwhile, top-5 accuracy is a per-token proxy metric you can cheaply track on‐the-fly to verify that your decoder is learning to place the correct word in its top candidates—even if it doesn’t always pick it first.

Creare fisiere de Train,Validate si Test:
Let’s walk through a tiny, fully‐numeric example so you can see exactly what each step is doing. Suppose you have 4 images, each with up to 3 human captions, and you choose:
    caps_per_img = 2
    min_freq = 1 (so every word that appears at least once stays in the vocab)
    max_length = 4

And your raw captions look like this:
Image File	Raw Captions
img1.jpg	“A cat.”
“Black cat.”
“Cat on mat.”
img2.jpg	“A dog.”
“Brown dog.”
img3.jpg	“A bird.”
“Blue bird.”
“Bird flies.”
img4.jpg	“A fish.”

1. Build caption_map and word_freq
You iterate through each line, tokenize, and count frequencies:
caption_map = {
  'img1.jpg': [['a','cat'], ['black','cat'], ['cat','on','mat']],
  'img2.jpg': [['a','dog'], ['brown','dog']],
  'img3.jpg': [['a','bird'], ['blue','bird'], ['bird','flies']],
  'img4.jpg': [['a','fish']],
}
word_freq = Counter({
  'a':4, 'cat':3, 'black':1, 'on':1, 'mat':1,
  'dog':2, 'brown':1,
  'bird':3, 'blue':1, 'flies':1,
  'fish':1
})

2. Split 80/10/10 by image
With seed(28) you shuffle
img_list = ['img1.jpg','img2.jpg','img3.jpg','img4.jpg'] → say it becomes
['img3.jpg','img1.jpg','img4.jpg','img2.jpg'].
    train_end = int(0.8*4) = 3
    val_end = int(0.9*4) = 3 (so val is just the 4th image)
Resulting split_map:
Filename	Split
img3.jpg	train
img1.jpg	train
img4.jpg	train
img2.jpg	test
(no val because 0.94==3)*	
(If you want exactly 1 image for val, you’d do 75/10/15 or similar. This small‐N edge case shows why tiny toy datasets need careful ratio choice.)

3. Build your word_map
Keeping words with freq > min_freq=1 gives ['a','cat','dog','bird'] (since all freq > 1). You enumerate them:
index_map = {
  'a':    1,
  'cat':  2,
  'dog':  3,
  'bird': 4,
  '<pad>':0,
  '<start>':5,
  '<end>':6,
  '<unk>':7,
}

4. Writing HDF5 & JSON for TRAIN
For split='train', images ['img3.jpg','img1.jpg','img4.jpg'], captions:
    img3: [['a','bird'],['blue','bird'],['bird','flies']]
    img1: [['a','cat'],['black','cat'],['cat','on','mat']]
    img4: [['a','fish']]
Because caps_per_img=2, you sample or duplicate:
    img3: you randomly pick 2 of its 3 captions, say [['a','bird'],['bird','flies']].
    img1: pick 2 of 3, say [['a','cat'],['black','cat']].
    img4: only 1 caption so duplicate it → [['a','fish'],['a','fish']].
You then resize each image to (3,256,256) and store in the HDF5 dataset of shape (3,3,256,256).

5. Encode & pad captions
For each chosen caption you build:
    [<start>] + indices + [<end>] + <pad>… up to length max_length=4.
    Record the true length = len(tokens) + 2.
Let’s do img3’s two picks:
    Caption ['a','bird']
        indices: [1,4]
        with start/end/pad → [5,1,4,6] (exactly 4 tokens, no pad)
        length = 2 + 2 = 4
    Caption ['bird','flies']
        “flies” isn’t in index_map → <unk> = 7
        [<start>=5, 4, 7, <end>=6] → [5,4,7,6]
        length = 2 + 2 = 4

And so on for each image and its 2 sampled captions. You end up with:
    encoded_caps list of length #images * caps_per_img = 3*2 = 6, each a 4-length list.
    cap_lengths list of length 6, each value 4.

These two lists get written to JSON files:
TRAIN_CAPTIONS_*.json  → [[5,1,4,6],[5,4,7,6], … 6 lists total]
TRAIN_CAPLENS_*.json    → [4,4, … 6 values]

BLEU?
BLEU (Bilingual Evaluation Understudy) is a precision-based automatic metric for evaluating the quality of generated text (e.g. machine translation, image captions) against one or more reference texts. Although originally developed for translation, it’s widely used in image captioning to compare your model’s captions to ground-truth human captions.

Beam Search
https://en.wikipedia.org/wiki/Beam_search



Această funcție train realizează un întreg epoch de învăţare pentru modelul tău de image captioning, parcurgând batch-urile de imagini şi caption-uri, făcând forward pass prin encoder şi decoder, calculând loss-ul de cross-entropy plus penalizarea atenţiei (pentru a încuraja acoperirea completă a imaginii), apoi aplicând backward pass cu gradient clipping şi actualizând parametrii ambelor reţele (encoder şi decoder) prin optimizatori. În paralel, ea măsoară timpii de încărcare şi de procesare, loss-ul curent şi mediu, precum şi acurateţea top-5, şi tipăreşte la intervale regulate starea curentă şi statisticile medii pentru a monitoriza progresul antrenării.

Funcția validate parcurge setul de validare pas cu pas, trece imaginile prin encoder și decoder pentru a obține scoruri pe vocabular, calculează loss-ul (inclusiv regularizarea atenției), măsoară acuratețea top-5 și timpii de procesare, iar în paralel construiește două liste: references (toate caption-urile umane de referință, filtrate de token-urile <start> și <pad>) și hypotheses (caption-urile generate de model, tăiate la lungimile reale). La final, folosește aceste liste pentru a calcula și afișa scorul BLEU-4, apoi returnează acest BLEU-4 pentru comparație între epoci sau configurări de model.